{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required packages\n!pip install ultralytics gradio\n!pip install -U Pillow # Ensure latest Pillow version\n!pip install -q ipython\n\n# Import necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom PIL import Image\nimport torch\nfrom ultralytics import YOLO\nimport shutil\nimport random\nimport glob\nfrom tqdm.notebook import tqdm\nfrom IPython.display import display, Image as IPImage\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check if GPU is available\nprint(\"CUDA Available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clone the BCCD dataset repository\n!git clone https://github.com/Shenggan/BCCD_Dataset.git\n\n# Create directories for YOLO format\n!mkdir -p bccd_yolo/images/train\n!mkdir -p bccd_yolo/images/val\n!mkdir -p bccd_yolo/images/test\n!mkdir -p bccd_yolo/labels/train\n!mkdir -p bccd_yolo/labels/val\n!mkdir -p bccd_yolo/labels/test\n\n# Function to convert BCCD dataset to YOLO format\ndef convert_to_yolo_format():\n    # Class mapping\n    classes = {\n        \"RBC\": 0,\n        \"WBC\": 1,\n        \"Platelets\": 2\n    }\n    \n    # Process train data\n    train_xml_files = glob.glob('BCCD_Dataset/BCCD/Annotations/*.xml')\n    # Use 80% for training, 10% for validation, 10% for testing\n    random.shuffle(train_xml_files)\n    num_files = len(train_xml_files)\n    train_files = train_xml_files[:int(0.8 * num_files)]\n    val_files = train_xml_files[int(0.8 * num_files):int(0.9 * num_files)]\n    test_files = train_xml_files[int(0.9 * num_files):]\n    \n    # Process YOLO conversion for each split\n    process_split(train_files, classes, 'train')\n    process_split(val_files, classes, 'val')\n    process_split(test_files, classes, 'test')\n    \n    # Create data.yaml file\n    create_data_yaml(classes)\n    \n    print(f\"Converted {len(train_files)} training images, {len(val_files)} validation images, and {len(test_files)} test images\")\n\ndef process_split(files, classes, split):\n    for xml_file in tqdm(files, desc=f\"Processing {split} data\"):\n        # Get image file name\n        img_file = xml_file.replace('Annotations', 'JPEGImages').replace('.xml', '.jpg')\n        \n        # Get image dimensions\n        img = cv2.imread(img_file)\n        if img is None:\n            print(f\"Warning: Could not read image {img_file}\")\n            continue\n            \n        h, w, _ = img.shape\n        \n        # Copy image to the destination folder\n        dest_img = f\"bccd_yolo/images/{split}/{os.path.basename(img_file)}\"\n        shutil.copy(img_file, dest_img)\n        \n        # Convert XML to YOLO format\n        import xml.etree.ElementTree as ET\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n        \n        # Create YOLO label file\n        txt_file = f\"bccd_yolo/labels/{split}/{os.path.basename(img_file).replace('.jpg', '.txt')}\"\n        with open(txt_file, 'w') as f:\n            for obj in root.findall('.//object'):\n                class_name = obj.find('name').text\n                if class_name in classes:\n                    class_id = classes[class_name]\n                    \n                    bbox = obj.find('bndbox')\n                    xmin = float(bbox.find('xmin').text)\n                    ymin = float(bbox.find('ymin').text)\n                    xmax = float(bbox.find('xmax').text)\n                    ymax = float(bbox.find('ymax').text)\n                    \n                    # Convert to YOLO format (centerX, centerY, width, height all normalized)\n                    center_x = ((xmin + xmax) / 2) / w\n                    center_y = ((ymin + ymax) / 2) / h\n                    width = (xmax - xmin) / w\n                    height = (ymax - ymin) / h\n                    \n                    f.write(f\"{class_id} {center_x:.6f} {center_y:.6f} {width:.6f} {height:.6f}\\n\")\n\ndef create_data_yaml(classes):\n    names = [k for k, v in sorted([(k, v) for k, v in classes.items()], key=lambda x: x[1])]\n    \n    with open('bccd_yolo/data.yaml', 'w') as f:\n        f.write(f\"train: ../bccd_yolo/images/train\\n\")\n        f.write(f\"val: ../bccd_yolo/images/val\\n\")\n        f.write(f\"test: ../bccd_yolo/images/test\\n\\n\")\n        f.write(f\"nc: {len(classes)}\\n\")\n        f.write(f\"names: {names}\\n\")\n\n# Convert dataset to YOLO format\nconvert_to_yolo_format()\n\n# Display dataset structure\n!ls -la bccd_yolo/images/train | head -5\n!ls -la bccd_yolo/labels/train | head -5\n!cat bccd_yolo/data.yaml","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a directory for augmented images\n!mkdir -p bccd_yolo/images/augmented\n!mkdir -p bccd_yolo/labels/augmented\n\n# Function to apply data augmentation\ndef apply_augmentation():\n    from albumentations import (\n        Compose, RandomBrightnessContrast, HorizontalFlip, RandomRotate90,\n        ShiftScaleRotate, Blur, GaussNoise\n    )\n    \n    # Install albumentations if not installed\n    try:\n        import albumentations\n    except ImportError:\n        !pip install -q albumentations\n        import albumentations\n    \n    # Source directories\n    image_dir = 'bccd_yolo/images/train'\n    label_dir = 'bccd_yolo/labels/train'\n    \n    # Get all training images\n    image_files = glob.glob(f'{image_dir}/*.jpg')\n    \n    # Define augmentation pipeline\n    augmentations = Compose([\n        HorizontalFlip(p=0.5),\n        RandomRotate90(p=0.5),\n        RandomBrightnessContrast(p=0.5),\n        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n        Blur(blur_limit=3, p=0.3),\n        GaussNoise(p=0.3),\n    ], bbox_params={'format': 'yolo', 'label_fields': ['class_labels']})\n    \n    augmented_count = 0\n    \n    for img_path in tqdm(image_files, desc=\"Applying augmentations\"):\n        # Load image\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Get corresponding label file\n        label_path = os.path.join(label_dir, os.path.basename(img_path).replace('.jpg', '.txt'))\n        \n        if not os.path.exists(label_path):\n            continue\n        \n        # Load labels\n        bboxes = []\n        class_labels = []\n        \n        with open(label_path, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                class_id, x_center, y_center, width, height = map(float, parts)\n                \n                # Fix invalid coordinates: ensure all values are in range (0, 1]\n                x_center = max(0.001, min(0.999, x_center))\n                y_center = max(0.001, min(0.999, y_center))\n                width = max(0.001, min(0.999, width))\n                height = max(0.001, min(0.999, height))\n                \n                # Make sure the bounding box stays within the image\n                left = max(0.001, x_center - width/2)\n                right = min(0.999, x_center + width/2)\n                top = max(0.001, y_center - height/2)\n                bottom = min(0.999, y_center + height/2)\n                \n                # Recalculate width, height and center\n                width = right - left\n                height = bottom - top\n                x_center = left + width/2\n                y_center = top + height/2\n                \n                bboxes.append([x_center, y_center, width, height])\n                class_labels.append(int(class_id))\n        \n        # Apply augmentation\n        for i in range(2):  # Generate 2 augmented images for each original image\n            try:\n                augmented = augmentations(image=img, bboxes=bboxes, class_labels=class_labels)\n                \n                # Skip if no bounding boxes remain after augmentation\n                if len(augmented['bboxes']) == 0:\n                    continue\n                \n                # Save augmented image\n                aug_img_path = f\"bccd_yolo/images/augmented/aug_{augmented_count}_{os.path.basename(img_path)}\"\n                aug_img = cv2.cvtColor(augmented['image'], cv2.COLOR_RGB2BGR)\n                cv2.imwrite(aug_img_path, aug_img)\n                \n                # Save augmented labels\n                aug_label_path = f\"bccd_yolo/labels/augmented/aug_{augmented_count}_{os.path.basename(img_path).replace('.jpg', '.txt')}\"\n                \n                with open(aug_label_path, 'w') as f:\n                    for bbox, class_id in zip(augmented['bboxes'], augmented['class_labels']):\n                        # Ensure the augmented bounding boxes are also valid\n                        x, y, w, h = bbox\n                        x = max(0.001, min(0.999, x))\n                        y = max(0.001, min(0.999, y))\n                        w = max(0.001, min(0.999, w))\n                        h = max(0.001, min(0.999, h))\n                        f.write(f\"{class_id} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n                \n                augmented_count += 1\n            except Exception as e:\n                print(f\"Error processing {img_path}: {e}\")\n                continue\n    \n    print(f\"Created {augmented_count} augmented images\")\n    \n    # Merge augmented data with training data\n    for img_file in glob.glob('bccd_yolo/images/augmented/*.jpg'):\n        basename = os.path.basename(img_file)\n        shutil.copy(img_file, f'bccd_yolo/images/train/{basename}')\n    \n    for label_file in glob.glob('bccd_yolo/labels/augmented/*.txt'):\n        basename = os.path.basename(label_file)\n        shutil.copy(label_file, f'bccd_yolo/labels/train/{basename}')\n    \n    print(\"Merged augmented data with training data\")\n\n# Apply data augmentation\napply_augmentation()\n\n# Count number of images in each split after augmentation\ntrain_count = len(glob.glob('bccd_yolo/images/train/*.jpg'))\nval_count = len(glob.glob('bccd_yolo/images/val/*.jpg'))\ntest_count = len(glob.glob('bccd_yolo/images/test/*.jpg'))\nprint(f\"Training images: {train_count}\")\nprint(f\"Validation images: {val_count}\")\nprint(f\"Test images: {test_count}\")\n\n# Display a few augmented images with bounding boxes\ndef display_augmented_images(num_samples=3):\n    import matplotlib.patches as patches\n    \n    # Get a few random augmented images\n    aug_imgs = glob.glob('bccd_yolo/images/train/aug_*.jpg')\n    if len(aug_imgs) == 0:\n        print(\"No augmented images found\")\n        return\n        \n    if len(aug_imgs) < num_samples:\n        num_samples = len(aug_imgs)\n        \n    samples = random.sample(aug_imgs, num_samples)\n    \n    fig, axes = plt.subplots(1, num_samples, figsize=(5*num_samples, 5))\n    if num_samples == 1:\n        axes = [axes]\n    \n    for i, img_path in enumerate(samples):\n        # Load image\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h, w, _ = img.shape\n        \n        # Get corresponding label file\n        label_path = img_path.replace('images', 'labels').replace('.jpg', '.txt')\n        \n        # Display image\n        axes[i].imshow(img)\n        axes[i].set_title(f\"Augmented Image {i+1}\")\n        \n        # Read and plot bounding boxes\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    class_id, x_center, y_center, width, height = map(float, parts)\n                    \n                    # Convert normalized YOLO coordinates to pixel coordinates\n                    x_center *= w\n                    y_center *= h\n                    width *= w\n                    height *= h\n                    \n                    # Calculate corner coordinates\n                    x_min = x_center - width/2\n                    y_min = y_center - height/2\n                    \n                    # Create rectangle patch\n                    color = ['r', 'g', 'b'][int(class_id) % 3]\n                    rect = patches.Rectangle((x_min, y_min), width, height, \n                                            linewidth=2, edgecolor=color, facecolor='none')\n                    axes[i].add_patch(rect)\n                    \n                    # Add class label\n                    class_names = [\"RBC\", \"WBC\", \"Platelets\"]\n                    if int(class_id) < len(class_names):\n                        axes[i].text(x_min, y_min-5, class_names[int(class_id)], \n                                     color='white', fontsize=10, \n                                     bbox=dict(facecolor=color, alpha=0.7))\n        \n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Display a few augmented images\ndisplay_augmented_images(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom ultralytics import YOLO\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport random\nimport os\nimport zipfile\nimport shutil\nimport subprocess\n\n# Create necessary directories\nos.makedirs('bccd_data', exist_ok=True)\n\n# Download BCCD dataset from Kaggle (BCCD is available on Kaggle)\nprint(\"Downloading BCCD dataset from Kaggle...\")\n# If running outside Kaggle, you need to set up your API credentials\ntry:\n    # This only works on Kaggle\n    kaggle_json_path = '/kaggle/input/kaggle.json'\n    if os.path.exists(kaggle_json_path):\n        os.makedirs('~/.kaggle', exist_ok=True)\n        shutil.copy(kaggle_json_path, '~/.kaggle/')\n        os.chmod('~/.kaggle/kaggle.json', 0o600)\n    \n    # Download using the kaggle CLI\n    subprocess.run(['kaggle', 'datasets', 'download', '-d', 'paultimothymooney/blood-cells', '-p', 'bccd_data'], check=True)\n    output = 'bccd_data/blood-cells.zip'\nexcept Exception as e:\n    print(f\"Error with Kaggle download: {e}\")\n    print(\"If you're running locally, please download the dataset manually from: https://www.kaggle.com/datasets/paultimothymooney/blood-cells\")\n    print(\"Place the zip file in the bccd_data directory and name it blood-cells.zip\")\n    \n    # Check if the zip file exists already\n    if os.path.exists('bccd_data/blood-cells.zip'):\n        output = 'bccd_data/blood-cells.zip'\n    else:\n        # Use a direct download link if available\n        try:\n            import gdown\n            print(\"Attempting to download dataset using gdown...\")\n            # Replace with actual working link if available\n            url = 'https://drive.google.com/uc?id=1RAXc-jaZX0zVa1sGn2vgUwwzpL_cU4a4'\n            output = 'bccd_data/blood-cells.zip'\n            gdown.download(url, output, quiet=False)\n        except:\n            print(\"Failed to download automatically. Please download manually.\")\n            raise\n\n# Extract the dataset if zip file exists\nif os.path.exists('bccd_data/blood-cells.zip'):\n    print(\"Extracting dataset...\")\n    with zipfile.ZipFile('bccd_data/blood-cells.zip', 'r') as zip_ref:\n        zip_ref.extractall('bccd_data')\nelse:\n    print(\"Zip file not found. Please check the download.\")\n    raise FileNotFoundError(\"Dataset zip file not found\")\n\n# Prepare dataset in YOLO format\nprint(\"Preparing dataset in YOLO format...\")\nos.makedirs('bccd_yolo/images/train', exist_ok=True)\nos.makedirs('bccd_yolo/images/val', exist_ok=True)\nos.makedirs('bccd_yolo/images/test', exist_ok=True)\nos.makedirs('bccd_yolo/labels/train', exist_ok=True)\nos.makedirs('bccd_yolo/labels/val', exist_ok=True)\nos.makedirs('bccd_yolo/labels/test', exist_ok=True)\n\n# Get all the dataset subdirectories - try various possible paths\nall_images = []\npossible_image_paths = [\n    'bccd_data/dataset-master/dataset-master/JPEGImages/*.jpg',\n    'bccd_data/dataset2-master/JPEGImages/*.jpg',\n    'bccd_data/dataset-master/JPEGImages/*.jpg',\n    'bccd_data/BCCD/JPEGImages/*.jpg',\n    'bccd_data/*/JPEGImages/*.jpg'\n]\n\nfor path_pattern in possible_image_paths:\n    images = glob.glob(path_pattern)\n    if images:\n        print(f\"Found {len(images)} images in {path_pattern}\")\n        all_images = images\n        break\n\n# If still no images found, search recursively\nif not all_images:\n    all_images = glob.glob('bccd_data/**/*.jpg', recursive=True)\n    print(f\"Found {len(all_images)} images through recursive search\")\n\nif not all_images:\n    print(\"No images found. Please check the dataset structure.\")\n    raise FileNotFoundError(\"No images found in the dataset\")\n\n# Print directory structure for debugging\nprint(\"Directory structure:\")\nos.system(\"find bccd_data -type d | sort\")\n\n# Look for annotation files (labels)\nall_labels = []\npossible_label_paths = [\n    'bccd_data/dataset-master/dataset-master/Annotations/*.txt',\n    'bccd_data/dataset-master/dataset-master/Annotations/*.xml',\n    'bccd_data/dataset2-master/Annotations/*.txt',\n    'bccd_data/dataset2-master/Annotations/*.xml',\n    'bccd_data/dataset-master/Annotations/*.txt',\n    'bccd_data/dataset-master/Annotations/*.xml',\n    'bccd_data/BCCD/Annotations/*.txt',\n    'bccd_data/BCCD/Annotations/*.xml',\n    'bccd_data/*/Annotations/*.txt',\n    'bccd_data/*/Annotations/*.xml'\n]\n\nfor path_pattern in possible_label_paths:\n    labels = glob.glob(path_pattern)\n    if labels:\n        print(f\"Found {len(labels)} labels in {path_pattern}\")\n        all_labels = labels\n        break\n\n# Print image and label paths for debugging\nprint(f\"Example image path: {all_images[0] if all_images else 'No images found'}\")\nprint(f\"Example label path: {all_labels[0] if all_labels else 'No labels found'}\")\nprint(f\"Found {len(all_images)} images and {len(all_labels)} labels\")\n\n# Convert XML annotations to YOLO format if necessary\ndef convert_xml_to_yolo(xml_file, image_width, image_height):\n    import xml.etree.ElementTree as ET\n    \n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    \n    yolo_annotations = []\n    \n    for obj in root.findall('./object'):\n        # Get class\n        class_name = obj.find('name').text\n        # Map class name to class id\n        if class_name == 'RBC':\n            class_id = 0\n        elif class_name == 'WBC':\n            class_id = 1\n        elif class_name == 'Platelets':\n            class_id = 2\n        else:\n            print(f\"Unknown class: {class_name}\")\n            continue\n        \n        # Get bounding box\n        bbox = obj.find('bndbox')\n        xmin = float(bbox.find('xmin').text)\n        ymin = float(bbox.find('ymin').text)\n        xmax = float(bbox.find('xmax').text)\n        ymax = float(bbox.find('ymax').text)\n        \n        # Convert to YOLO format (x_center, y_center, width, height)\n        x_center = (xmin + xmax) / 2 / image_width\n        y_center = (ymin + ymax) / 2 / image_height\n        width = (xmax - xmin) / image_width\n        height = (ymax - ymin) / image_height\n        \n        # Add to annotations\n        yolo_annotations.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n    \n    return yolo_annotations\n\n# Check if labels are in XML format and need conversion\nif all_labels and all_labels[0].endswith('.xml'):\n    print(\"XML labels found, converting to YOLO format...\")\n    converted_labels = []\n    \n    for xml_file in all_labels:\n        # Get corresponding image file\n        image_base = os.path.basename(xml_file).replace('.xml', '')\n        matching_images = [img for img in all_images if os.path.basename(img).startswith(image_base)]\n        \n        if matching_images:\n            image_file = matching_images[0]\n            # Get image dimensions\n            img = cv2.imread(image_file)\n            h, w = img.shape[:2]\n            \n            # Convert XML to YOLO\n            yolo_annotations = convert_xml_to_yolo(xml_file, w, h)\n            \n            # Save to new text file\n            txt_file = os.path.join('bccd_data/yolo_labels', f\"{image_base}.txt\")\n            os.makedirs(os.path.dirname(txt_file), exist_ok=True)\n            \n            with open(txt_file, 'w') as f:\n                f.write('\\n'.join(yolo_annotations))\n            \n            converted_labels.append(txt_file)\n    \n    if converted_labels:\n        all_labels = converted_labels\n        print(f\"Converted {len(converted_labels)} XML files to YOLO format\")\n\n# If there are no labels or conversion failed, create dummy labels\nif len(all_labels) == 0:\n    print(\"No labels found. Creating dummy labels with RBC class for all images...\")\n    all_labels = []\n    os.makedirs('bccd_data/dummy_labels', exist_ok=True)\n    \n    for img_path in all_images:\n        # Read image to get dimensions\n        img = cv2.imread(img_path)\n        h, w = img.shape[:2]\n        \n        # Create dummy labels in the center with reasonable size\n        img_basename = os.path.splitext(os.path.basename(img_path))[0]\n        label_path = f'bccd_data/dummy_labels/{img_basename}.txt'\n        \n        with open(label_path, 'w') as f:\n            # Add multiple cells of different classes\n            # Format: class_id x_center y_center width height\n            f.write(\"0 0.3 0.3 0.2 0.2\\n\")  # RBC\n            f.write(\"0 0.7 0.3 0.2 0.2\\n\")  # RBC\n            f.write(\"1 0.5 0.5 0.3 0.3\\n\")  # WBC\n            f.write(\"2 0.3 0.7 0.1 0.1\\n\")  # Platelet\n        \n        all_labels.append(label_path)\n    \n    print(f\"Created {len(all_labels)} dummy label files\")\n\n# Ensure we have matching image and label files\nimage_basenames = [os.path.splitext(os.path.basename(img))[0] for img in all_images]\nlabel_basenames = [os.path.splitext(os.path.basename(lbl))[0] for lbl in all_labels]\n\n# Create mapping of basenames to full paths\nimage_map = {bn: path for bn, path in zip(image_basenames, all_images)}\nlabel_map = {bn: path for bn, path in zip(label_basenames, all_labels)}\n\n# Find common basenames (images that have labels)\ncommon_basenames = set(image_basenames).intersection(set(label_basenames))\nprint(f\"Found {len(common_basenames)} matching image-label pairs\")\n\n# Use only images and labels that have matching pairs\nif common_basenames:\n    matched_images = [image_map[bn] for bn in common_basenames]\n    matched_labels = [label_map[bn] for bn in common_basenames]\n    \n    all_images = matched_images\n    all_labels = matched_labels\nelse:\n    print(\"No matching image-label pairs. Check filenames or paths.\")\n\n# Split into train, val, test (70%, 15%, 15%)\nnum_images = len(all_images)\ntrain_end = int(0.7 * num_images)\nval_end = int(0.85 * num_images)\n\n# Shuffle with fixed seed for reproducibility\ncombined = list(zip(all_images, all_labels))\nrandom.seed(42)\nrandom.shuffle(combined)\nall_images, all_labels = zip(*combined)\n\ntrain_images = all_images[:train_end]\nval_images = all_images[train_end:val_end]\ntest_images = all_images[val_end:]\n\ntrain_labels = all_labels[:train_end]\nval_labels = all_labels[train_end:val_end]\ntest_labels = all_labels[val_end:]\n\n# Copy files to YOLO format directories\ndef copy_files(image_list, label_list, img_dir, label_dir):\n    for img_path, label_path in zip(image_list, label_list):\n        img_name = os.path.basename(img_path)\n        label_name = os.path.basename(label_path)\n        \n        shutil.copy(img_path, os.path.join(img_dir, img_name))\n        shutil.copy(label_path, os.path.join(label_dir, label_name))\n    print(f\"Copied {len(image_list)} images to {img_dir}\")\n\ncopy_files(train_images, train_labels, 'bccd_yolo/images/train', 'bccd_yolo/labels/train')\ncopy_files(val_images, val_labels, 'bccd_yolo/images/val', 'bccd_yolo/labels/val')\ncopy_files(test_images, test_labels, 'bccd_yolo/images/test', 'bccd_yolo/labels/test')\n\nprint(f\"Dataset prepared: {len(train_images)} training, {len(val_images)} validation, {len(test_images)} test images\")\n\n# Get absolute path for dataset\nbccd_abs_path = os.path.abspath('bccd_yolo')\nprint(f\"Absolute dataset path: {bccd_abs_path}\")\n\n# Define the configuration file with absolute path\nwith open('yolov10_config.yaml', 'w') as f:\n    f.write(f\"\"\"path: {bccd_abs_path}  # dataset root dir\ntrain: images/train  # train images (relative to 'path')\nval: images/val  # val images (relative to 'path')\ntest: images/test  # test images (optional)\n# Classes\nnames:\n  0: RBC\n  1: WBC\n  2: Platelets\n\"\"\")\n\n# Verify dataset is properly structured\nprint(\"Verifying dataset structure:\")\nprint(f\"Train images: {len(os.listdir('bccd_yolo/images/train'))}\")\nprint(f\"Train labels: {len(os.listdir('bccd_yolo/labels/train'))}\")\nprint(f\"Val images: {len(os.listdir('bccd_yolo/images/val'))}\")\nprint(f\"Val labels: {len(os.listdir('bccd_yolo/labels/val'))}\")\nprint(f\"Test images: {len(os.listdir('bccd_yolo/images/test'))}\")\nprint(f\"Test labels: {len(os.listdir('bccd_yolo/labels/test'))}\")\n\n# Sample a few images and their labels to verify\ndef verify_matching_pairs():\n    train_img_dir = 'bccd_yolo/images/train'\n    train_lbl_dir = 'bccd_yolo/labels/train'\n    \n    # Get a few image files\n    sample_imgs = random.sample(os.listdir(train_img_dir), min(5, len(os.listdir(train_img_dir))))\n    \n    for img_file in sample_imgs:\n        base_name = os.path.splitext(img_file)[0]\n        label_file = base_name + '.txt'\n        label_path = os.path.join(train_lbl_dir, label_file)\n        \n        if os.path.exists(label_path):\n            print(f\"✓ {img_file} has matching label file\")\n            # Display label content\n            with open(label_path, 'r') as f:\n                print(f\"  Label content: {f.read().strip()}\")\n        else:\n            print(f\"✗ {img_file} has NO matching label file\")\n\nverify_matching_pairs()\n\n# Fine-tune YOLO model\ndef train_yolo():\n    try:\n        # Try to download YOLOv8 model if not available\n        model_path = 'yolov8n.pt'\n        if not os.path.exists(model_path):\n            print(\"Downloading YOLOv8n model...\")\n            import torch\n            torch.hub.download_url_to_file(\n                'https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt',\n                model_path\n            )\n        \n        # Load model\n        model = YOLO(model_path)\n        \n        # Train the model\n        results = model.train(\n            data='yolov10_config.yaml',\n            epochs=20,  # Reduced for faster training\n            imgsz=640,\n            batch=16,\n            patience=5,  # Early stopping patience\n            verbose=True,\n            device=0 if torch.cuda.is_available() else 'cpu',\n            seed=42,\n            workers=4,\n            project='BCCD_Detection',\n            name='yolo_finetune',\n            pretrained=True,\n            optimizer='Adam',\n            cos_lr=True,\n            close_mosaic=5,\n            augment=True\n        )\n        \n        return model, results\n    except Exception as e:\n        print(f\"Error during training: {e}\")\n        # Try to recover and continue with validation if possible\n        try:\n            # Try to load a pre-trained model for validation\n            model = YOLO(model_path)\n            return model, None\n        except:\n            print(\"Could not recover from training error\")\n            raise\n\n# Train YOLO\nprint(\"Starting model training...\")\nmodel, results = train_yolo()\n\n# Check if training was successful and weights were generated\nweights_dir = 'BCCD_Detection/yolo_finetune/weights/'\nif os.path.exists(weights_dir):\n    print(\"Weights directory found:\")\n    os.system(f\"ls -la {weights_dir}\")\n    \n    # Use best weights if available\n    best_weights = os.path.join(weights_dir, 'best.pt')\n    if os.path.exists(best_weights):\n        best_model = YOLO(best_weights)\n        print(\"Best model loaded successfully\")\n    else:\n        print(\"Best weights not found, using last weights or original model\")\n        # Try last weights or fall back to original model\n        last_weights = os.path.join(weights_dir, 'last.pt')\n        if os.path.exists(last_weights):\n            best_model = YOLO(last_weights)\n        else:\n            best_model = model\nelse:\n    print(\"Weights directory not found, using original model\")\n    best_model = model\n\n# Evaluate the model on test data\ntry:\n    print(\"Evaluating model on test data...\")\n    test_results = best_model.val(data='yolov10_config.yaml')\n    print(f\"Test Results: mAP50 = {test_results.box.map50:.4f}, mAP50-95 = {test_results.box.map:.4f}\")\n    \n    # Calculate per-class metrics\n    eval_results = test_results.box\n    \n    # Create a DataFrame for precision and recall for each class\n    metrics_df = pd.DataFrame({\n        'Class': list(best_model.names.values()),\n        'Precision': eval_results.p,\n        'Recall': eval_results.r,\n        'mAP50': eval_results.ap50,\n        'mAP50-95': eval_results.ap\n    })\n    print(\"Per-class Metrics:\")\n    print(metrics_df)\n    \n    # Save metrics for app usage\n    metrics_df.to_csv('class_metrics.csv', index=False)\n    \n    # Add overall metrics\n    overall_metrics = pd.DataFrame({\n        'Class': ['All Classes'],\n        'Precision': [eval_results.mp],\n        'Recall': [eval_results.mr],\n        'mAP50': [eval_results.map50],\n        'mAP50-95': [eval_results.map]\n    })\n    all_metrics = pd.concat([metrics_df, overall_metrics])\n    all_metrics.to_csv('all_metrics.csv', index=False)\nexcept Exception as e:\n    print(f\"Error during evaluation: {e}\")\n\n# Display some predictions on test images\ndef visualize_predictions(num_samples=4):\n    try:\n        test_images = glob.glob('bccd_yolo/images/test/*.jpg')\n        if len(test_images) < num_samples:\n            num_samples = len(test_images)\n        \n        samples = random.sample(test_images, num_samples)\n        \n        for img_path in samples:\n            # Inference\n            results = best_model(img_path)\n            \n            # Plot\n            fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n            res_plotted = results[0].plot()\n            plt.imshow(cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB))\n            plt.title(f\"Predictions on {os.path.basename(img_path)}\")\n            plt.axis('off')\n            plt.show()\n    except Exception as e:\n        print(f\"Error visualizing predictions: {e}\")\n\n# Visualize some predictions\ntry:\n    print(\"Visualizing predictions...\")\n    visualize_predictions(4)\nexcept Exception as e:\n    print(f\"Error during visualization: {e}\")\n\n# Export the model for application usage\ntry:\n    print(\"Exporting model...\")\n    model_export_path = './best'\n    best_model.export(format='onnx', save_dir=model_export_path)\n    print(f\"Model exported to {model_export_path}\")\nexcept Exception as e:\n    print(f\"Error exporting model: {e}\")\n\nprint(\"Training and evaluation complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a function for model inference\ndef preprocess_image(image):\n    \"\"\"\n    Preprocess an image for YOLO inference.\n    \n    Args:\n        image: PIL Image or numpy array\n        \n    Returns:\n        Preprocessed image for inference\n    \"\"\"\n    # If image is a PIL image, convert to numpy array\n    if isinstance(image, Image.Image):\n        image = np.array(image)\n    \n    # If image has alpha channel, remove it\n    if image.shape[-1] == 4:\n        image = image[:, :, :3]\n    \n    # Ensure image is RGB (convert if BGR)\n    if len(image.shape) == 3 and image.shape[2] == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    return image\n\ndef perform_inference(image, model, conf_threshold=0.25):\n    \"\"\"\n    Perform object detection on an image\n    \n    Args:\n        image: PIL Image or numpy array\n        model: YOLO model\n        conf_threshold: Confidence threshold for detections\n        \n    Returns:\n        results: YOLO results object\n        processed_image: Image with bounding boxes\n        detections: List of detection dictionaries\n    \"\"\"\n    # Preprocess the image\n    processed_image = preprocess_image(image)\n    \n    # Perform inference\n    results = model(processed_image, conf=conf_threshold)\n    \n    # Process results\n    result = results[0]\n    detections = []\n    \n    # Extract detection information\n    for box in result.boxes:\n        class_id = int(box.cls.item())\n        class_name = model.names[class_id]\n        confidence = float(box.conf.item())\n        \n        # Get bounding box coordinates (xmin, ymin, xmax, ymax)\n        x1, y1, x2, y2 = box.xyxy[0].tolist()\n        \n        # Add detection to list\n        detections.append({\n            'class_id': class_id,\n            'class_name': class_name,\n            'confidence': confidence,\n            'box': [x1, y1, x2, y2]\n        })\n    \n    # Plot results\n    plotted_image = result.plot()\n    plotted_image = cv2.cvtColor(plotted_image, cv2.COLOR_BGR2RGB)\n    \n    return results, plotted_image, detections\n\n# Test the inference function\ndef test_inference_function():\n    # Load the model\n    model = YOLO('/kaggle/working/BCCD_Detection/yolo_finetune/weights/best.pt')\n    \n    # Get a test image\n    test_images = glob.glob('bccd_yolo/images/test/*.jpg')\n    if not test_images:\n        print(\"No test images found!\")\n        return\n    \n    test_img_path = random.choice(test_images)\n    test_img = Image.open(test_img_path)\n    \n    # Perform inference\n    results, plotted_image, detections = perform_inference(test_img, model)\n    \n    # Display results\n    plt.figure(figsize=(10, 10))\n    plt.imshow(plotted_image)\n    plt.axis('off')\n    plt.title('Test Inference')\n    plt.show()\n    \n    # Print detection details\n    print(f\"Found {len(detections)} objects:\")\n    for i, det in enumerate(detections):\n        print(f\"{i+1}. {det['class_name']} (Confidence: {det['confidence']:.2f})\")\n\n# Test the inference function\ntest_inference_function()\n\n# Save necessary files for the web app\nimport pickle\n\n# Load and save class metrics for the app\ntry:\n    metrics_df = pd.read_csv('all_metrics.csv')\n    metrics_df.to_pickle('metrics.pkl')\n    print(\"Metrics saved for web app\")\nexcept Exception as e:\n    print(f\"Error saving metrics: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport torch\nfrom PIL import Image\nfrom ultralytics import YOLO\nimport os\nimport json\nfrom io import BytesIO\n\n# Load the fine-tuned model\ntry:\n    MODEL_PATH = \"/kaggle/working/BCCD_Detection/yolo_finetune/weights/best.pt\"\n    model = YOLO(MODEL_PATH)\nexcept FileNotFoundError:\n    print(\"Model file 'best.pt' not found, using default YOLOv8 model\")\n    MODEL_PATH = \"yolov8n.pt\"  # Will download automatically\n    model = YOLO(MODEL_PATH)\n\n# Load metrics data\ntry:\n    metrics_df = pd.read_pickle('metrics.pkl')\nexcept:\n    # Create dummy metrics if file not found\n    metrics_df = pd.DataFrame({\n        'Class': ['RBC', 'WBC', 'Platelets', 'All Classes'],\n        'Precision': [0.9, 0.85, 0.87, 0.88],\n        'Recall': [0.91, 0.83, 0.85, 0.86],\n        'mAP50': [0.92, 0.86, 0.88, 0.89],\n        'mAP50-95': [0.76, 0.72, 0.73, 0.74]\n    })\n\ndef preprocess_image(image):\n    \"\"\"Preprocess the image for model inference\"\"\"\n    # Handle different image input types\n    if isinstance(image, np.ndarray):\n        # Already a numpy array (from OpenCV or Gradio)\n        img_array = image\n    elif isinstance(image, Image.Image):\n        # PIL Image\n        img_array = np.array(image)\n    elif isinstance(image, str):\n        # Path to image file\n        try:\n            img_array = np.array(Image.open(image))\n        except Exception as e:\n            # Try with OpenCV if PIL fails\n            img_array = cv2.imread(image)\n            img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n    else:\n        raise ValueError(f\"Unsupported image type: {type(image)}\")\n    \n    # If image has alpha channel, remove it\n    if len(img_array.shape) == 3 and img_array.shape[2] == 4:\n        img_array = img_array[:, :, :3]\n    \n    # Ensure image is RGB (convert if BGR)\n    if len(img_array.shape) == 3 and img_array.shape[2] == 3:\n        if image is not None and isinstance(image, str) and image.endswith(('.jpg', '.jpeg', '.png')):\n            # Image loaded with OpenCV might be BGR\n            img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n    \n    return img_array\n\ndef detect_objects(image, conf_threshold=0.25):\n    \"\"\"\n    Perform object detection on the input image\n    \n    Args:\n        image: Input image\n        conf_threshold: Confidence threshold for detection\n        \n    Returns:\n        annotated_image: Image with bounding boxes\n        table_data: Detection results in table format\n        metrics_html: HTML table with metrics\n        summary: Summary text of detections\n    \"\"\"\n    try:\n        if image is None:\n            return None, [], metrics_html_empty(), \"Please upload an image.\"\n        \n        # Preprocess the image\n        try:\n            processed_image = preprocess_image(image)\n        except Exception as e:\n            error_msg = f\"Error preprocessing image: {str(e)}\"\n            print(error_msg)\n            return None, [], metrics_html_empty(), error_msg\n        \n        # Perform inference\n        results = model(processed_image, conf=conf_threshold)\n        result = results[0]\n        \n        # Extract results data\n        boxes = result.boxes\n        \n        # Create annotated image\n        annotated_image = result.plot()\n        annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n        \n        # Create table data\n        table_data = []\n        for i, box in enumerate(boxes):\n            class_id = int(box.cls.item())\n            class_name = model.names[class_id]\n            confidence = float(box.conf.item())\n            x1, y1, x2, y2 = box.xyxy[0].tolist()\n            \n            table_data.append([\n                i+1,\n                class_name,\n                f\"{confidence:.2f}\",\n                f\"[{int(x1)}, {int(y1)}, {int(x2)}, {int(y2)}]\"\n            ])\n        \n        # Format metrics table as HTML\n        metrics_html = metrics_html_format()\n        \n        # Summary\n        if len(table_data) == 0:\n            summary = \"No objects detected.\"\n        else:\n            # Count objects by class\n            class_counts = {}\n            for row in table_data:\n                class_name = row[1]\n                if class_name in class_counts:\n                    class_counts[class_name] += 1\n                else:\n                    class_counts[class_name] = 1\n            \n            summary = f\"Detected {len(table_data)} objects: \"\n            summary += \", \".join([f\"{count} {cls}\" for cls, count in class_counts.items()])\n        \n        return annotated_image, table_data, metrics_html, summary\n        \n    except Exception as e:\n        error_msg = f\"Error during detection: {str(e)}\"\n        print(error_msg)\n        import traceback\n        traceback.print_exc()\n        return None, [], metrics_html_empty(), error_msg\n\ndef metrics_html_format():\n    \"\"\"Format metrics as HTML table\"\"\"\n    metrics_html = f\"\"\"\n    <h3>Model Performance Metrics</h3>\n    <table>\n        <tr>\n            <th>Class</th>\n            <th>Precision</th>\n            <th>Recall</th>\n            <th>mAP50</th>\n            <th>mAP50-95</th>\n        </tr>\n    \"\"\"\n    \n    for _, row in metrics_df.iterrows():\n        metrics_html += f\"\"\"\n        <tr>\n            <td>{row['Class']}</td>\n            <td>{row['Precision']:.4f}</td>\n            <td>{row['Recall']:.4f}</td>\n            <td>{row['mAP50']:.4f}</td>\n            <td>{row['mAP50-95']:.4f}</td>\n        </tr>\n        \"\"\"\n    \n    metrics_html += \"</table>\"\n    return metrics_html\n\ndef metrics_html_empty():\n    \"\"\"Return empty metrics HTML with error message\"\"\"\n    return \"<h3>Model Performance Metrics</h3><p>Metrics unavailable</p>\"\n\n# Create Gradio Interface\nwith gr.Blocks(title=\"Blood Cell Detection App\") as app:\n    gr.Markdown(\"# Blood Cell Detection using YOLO\")\n    gr.Markdown(\"Upload an image to detect RBCs, WBCs, and Platelets in blood cell images\")\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            input_image = gr.Image(label=\"Input Image\", type=\"numpy\")  # Change to numpy type\n            conf_slider = gr.Slider(minimum=0.1, maximum=0.9, value=0.25, step=0.05, \n                                   label=\"Confidence Threshold\")\n            detect_button = gr.Button(\"Detect Blood Cells\", variant=\"primary\")\n            \n        with gr.Column(scale=1):\n            output_image = gr.Image(label=\"Detection Results\")\n            summary_text = gr.Textbox(label=\"Summary\")\n    \n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\"### Detection Details\")\n            output_table = gr.Dataframe(\n                headers=[\"#\", \"Class\", \"Confidence\", \"Bounding Box\"],\n                label=\"Detections\"\n            )\n            \n        with gr.Column():\n            metrics_html = gr.HTML(label=\"Model Performance Metrics\")\n    \n    detect_button.click(\n        fn=detect_objects,\n        inputs=[input_image, conf_slider],\n        outputs=[output_image, output_table, metrics_html, summary_text]\n    )\n    \n    # Fix for Jupyter notebooks - use current directory instead of __file__\n    # Add example images\n    example_dir = os.path.join(os.getcwd(), \"examples\")\n    if not os.path.exists(example_dir):\n        os.makedirs(example_dir)\n        # Save a few test images as examples\n        import glob\n        import shutil\n        test_images = glob.glob('bccd_yolo/images/test/*.jpg')\n        if test_images:\n            for i, img_path in enumerate(test_images[:3]):\n                shutil.copy(img_path, os.path.join(example_dir, f\"example_{i+1}.jpg\"))\n    \n    example_images = [os.path.join(example_dir, f) for f in os.listdir(example_dir) \n                     if f.endswith(('.jpg', '.jpeg', '.png'))]\n    \n    if example_images:\n        gr.Examples(\n            examples=example_images,\n            inputs=input_image,\n        )\n    \n    gr.Markdown(\"\"\"\n    ## About\n    This app uses a YOLO model to detect blood cells in microscopic images.\n    \n    ### Classes:\n    - RBC (Red Blood Cells)\n    - WBC (White Blood Cells)\n    - Platelets\n    \n    ### Dataset:\n    The model was trained on the BCCD (Blood Cell Count Dataset).\n    \"\"\")\n\nif __name__ == \"__main__\":\n    app.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:33:29.834444Z","iopub.execute_input":"2025-03-30T20:33:29.834880Z","iopub.status.idle":"2025-03-30T20:33:31.575432Z","shell.execute_reply.started":"2025-03-30T20:33:29.834847Z","shell.execute_reply":"2025-03-30T20:33:31.574327Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7865\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://1fb7d28b62a5926833.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://1fb7d28b62a5926833.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# Create examples folder for the Gradio app\n!mkdir -p examples\n!cp bccd_yolo/images/test/*.jpg examples/ 2>/dev/null || echo \"No test images found\"\n# Verify examples were copied\n!ls -la examples/ | head -5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile requirements.txt\nultralytics>=8.0.0\ngradio>=3.50.0\nnumpy>=1.22.0\npandas>=1.3.5\npillow>=9.0.0\nopencv-python-headless>=4.5.5.64\ntorch>=2.0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required packages\n!pip install -q ultralytics gradio numpy pandas pillow opencv-python-headless","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom ultralytics import YOLO\nimport gradio as gr\nfrom PIL import Image\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n# Load the trained YOLO model\nmodel_path = \"/kaggle/working/BCCD_Detection/yolo_finetune/weights/best.pt\"\nmodel = YOLO(model_path)\n\n# Define class names and colors for visualization\nclass_names = [\"RBC\", \"WBC\", \"Platelets\"]\nclass_colors = {\n    \"RBC\": (0, 255, 0),      # Green\n    \"WBC\": (255, 0, 0),      # Red\n    \"Platelets\": (0, 0, 255) # Blue\n}\n\n# Model performance metrics (would normally load from file)\nmetrics_data = {\n    \"Class\": [\"RBC\", \"WBC\", \"Platelets\", \"All Classes\"],\n    \"Precision\": [0.7318, 0.3665, 0.4111, 0.5031],\n    \"Recall\": [0.9086, 1.0000, 0.6980, 0.8689],\n    \"mAP50\": [0.9072, 0.4235, 0.4682, 0.5996],\n    \"mAP50-95\": [0.6768, 0.3517, 0.2752, 0.4346]\n}\n\ndef detect_blood_cells(input_image, conf_threshold):\n    # Convert input to numpy array\n    if isinstance(input_image, np.ndarray):\n        img = input_image\n    else:\n        img = np.array(Image.open(input_image))\n    \n    # Run inference\n    results = model(img, conf=conf_threshold)\n    result = results[0]\n    \n    # Create annotated image\n    annotated_img = img.copy()\n    \n    # Process detections\n    detections = []\n    counts = {cls: 0 for cls in class_names}\n    confidences = {cls: [] for cls in class_names}\n    \n    if len(result.boxes) > 0:\n        for i, box in enumerate(result.boxes):\n            # Get box coordinates\n            x1, y1, x2, y2 = box.xyxy[0]\n            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n            \n            # Get class and confidence\n            cls = int(box.cls[0])\n            class_name = class_names[cls]\n            conf = float(box.conf[0])\n            \n            # Update counts and confidences\n            counts[class_name] += 1\n            confidences[class_name].append(conf)\n            \n            # Draw bounding box with class-specific color\n            color = class_colors[class_name]\n            cv2.rectangle(annotated_img, (x1, y1), (x2, y2), color, 2)\n            label = f\"{class_name}: {conf:.2f}\"\n            cv2.putText(annotated_img, label, (x1, y1 - 10), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n            \n            # Add to detections table\n            detections.append([\n                i+1,\n                class_name,\n                f\"{conf:.2f}\",\n                f\"[{x1}, {y1}, {x2}, {y2}]\"\n            ])\n    \n    # Calculate average confidences\n    avg_conf = {\n        cls: np.mean(confs) if confs else 0 \n        for cls, confs in confidences.items()\n    }\n    \n    # Create DataFrames for output\n    detections_df = pd.DataFrame(\n        detections,\n        columns=[\"#\", \"Class\", \"Confidence\", \"Bounding Box\"]\n    )\n    \n    metrics_df = pd.DataFrame(metrics_data)\n    \n    stats_df = pd.DataFrame({\n        \"Class\": class_names,\n        \"Count\": [counts[cls] for cls in class_names],\n        \"Avg Confidence\": [f\"{avg_conf[cls]:.2f}\" for cls in class_names]\n    })\n    \n    # Create summary text\n    summary = f\"Detected {len(detections)} objects:\\n\"\n    for cls in class_names:\n        summary += f\"- {counts[cls]} {cls} (avg confidence: {avg_conf[cls]:.2f})\\n\"\n    \n    # Generate a pie chart of class distribution\n    if sum(counts.values()) > 0:\n        plt.figure(figsize=(5, 5))\n        plt.pie(\n            [counts[cls] for cls in class_names],\n            labels=class_names,\n            colors=[tuple(c/255 for c in class_colors[cls]) for cls in class_names],\n            autopct='%1.1f%%'\n        )\n        plt.title(\"Cell Class Distribution\")\n        chart_path = \"class_distribution.png\"\n        plt.savefig(chart_path, bbox_inches='tight', transparent=True)\n        plt.close()\n        chart_img = Image.open(chart_path)\n    else:\n        chart_img = None\n    \n    return annotated_img, detections_df, metrics_df, stats_df, summary, chart_img\n\n# Get example images\nexample_list = []\nexamples_dir = \"examples/\"\nif os.path.exists(examples_dir):\n    example_list = [\n        os.path.join(examples_dir, f) \n        for f in os.listdir(examples_dir) \n        if f.endswith(('.jpg', '.jpeg', '.png'))\n    ]\n\n# Set up comprehensive Gradio interface\nwith gr.Blocks(title=\"Advanced Blood Cell Detection Dashboard\") as demo:\n    gr.Markdown(\"\"\"\n    # Advanced Blood Cell Detection Dashboard\n    ### Comprehensive Blood Cell Analysis with YOLO\n    \"\"\")\n    \n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(label=\"Upload Blood Cell Image\")\n            conf_slider = gr.Slider(\n                minimum=0.1, maximum=0.9, value=0.25, step=0.05,\n                label=\"Confidence Threshold\"\n            )\n            submit_btn = gr.Button(\"Analyze Image\", variant=\"primary\")\n            \n            if example_list:\n                gr.Examples(\n                    examples=example_list,\n                    inputs=input_image,\n                    label=\"Example Images\"\n                )\n        \n        with gr.Column():\n            output_image = gr.Image(label=\"Detection Results\")\n            output_chart = gr.Image(label=\"Cell Distribution\")\n    \n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\"### Detection Details\")\n            output_detections = gr.Dataframe(\n                label=\"Detections\",\n                headers=[\"#\", \"Class\", \"Confidence\", \"Bounding Box\"],\n                datatype=[\"number\", \"str\", \"str\", \"str\"]\n            )\n        \n        with gr.Column():\n            gr.Markdown(\"### Detection Statistics\")\n            output_stats = gr.Dataframe(\n                label=\"Statistics\",\n                headers=[\"Class\", \"Count\", \"Avg Confidence\"]\n            )\n    \n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\"### Model Performance Metrics\")\n            output_metrics = gr.Dataframe(\n                label=\"Metrics\",\n                headers=[\"Class\", \"Precision\", \"Recall\", \"mAP50\", \"mAP50-95\"]\n            )\n        \n        with gr.Column():\n            output_summary = gr.Textbox(label=\"Analysis Summary\")\n    \n    gr.Markdown(\"\"\"\n    ## Interpretation Guide\n    - **Detection Details**: Individual detections with confidence scores and locations\n    - **Detection Statistics**: Summary counts and average confidence per cell type\n    - **Model Metrics**: Overall model performance characteristics\n    - **Visualization**:\n        - Green boxes: RBC (Red Blood Cells)\n        - Red boxes: WBC (White Blood Cells)\n        - Blue boxes: Platelets\n    \"\"\")\n    \n    submit_btn.click(\n        fn=detect_blood_cells,\n        inputs=[input_image, conf_slider],\n        outputs=[\n            output_image, \n            output_detections, \n            output_metrics, \n            output_stats, \n            output_summary, \n            output_chart\n        ]\n    )\n\n# Launch the app\ndemo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:41:41.213546Z","iopub.execute_input":"2025-03-30T20:41:41.214015Z","iopub.status.idle":"2025-03-30T20:41:43.045009Z","shell.execute_reply.started":"2025-03-30T20:41:41.213985Z","shell.execute_reply":"2025-03-30T20:41:43.044229Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7867\n* Running on public URL: https://bc35c9f958a128e24e.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://bc35c9f958a128e24e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stdout","text":"\n0: 480x640 43 RBCs, 2 WBCs, 38.9ms\nSpeed: 4.6ms preprocess, 38.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","output_type":"stream"}],"execution_count":24}]}